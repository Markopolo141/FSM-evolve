\documentclass{article}
\usepackage{amsmath}
% \title{Perturbation solution}
% \author{Mark Burgess}

\newcommand{\rvline}{\hspace*{-\arraycolsep}\vline\hspace*{-\arraycolsep}}
% https://tex.stackexchange.com/questions/323297/typing-block-matrices-with-zero-blocks-and-seperators

\begin{document}
% \maketitle

We consider a species, with a population Matrix $A$, which is parameterised by genomic perturbation $\epsilon$, and a superpopulation vector $Y$. We consider the change in the largest eigenvector/value $X,\lambda$ as a function of $\epsilon$.
$X+X'\epsilon$,$\lambda+\lambda'\epsilon$ to first order of $\epsilon$. (using Einstein summation notation):

$$ \lambda^1(\epsilon^1)X_{i}^1(\epsilon^1) = A(\epsilon^1,\sum_k\alpha^k(\epsilon^1)X^k(\epsilon^1))_{i,m}X_{m}^1(\epsilon^1) $$
$$ \lambda^2(\epsilon^2)X_{i}^2(\epsilon^2) = A(\epsilon^2,\sum_k\alpha^k(\epsilon^2)X^k(\epsilon^2))_{i,m}X_{m}^2(\epsilon^2) $$
$$ \lambda^3(\epsilon^3)X_{i}^3(\epsilon^3) = A(\epsilon^3,\sum_k\alpha^k(\epsilon^3)X^k(\epsilon^3))_{i,m}X_{m}^3(\epsilon^3) $$
$$ \lambda^4(\epsilon^4)X_{i}^4(\epsilon^4) = A(\epsilon^4,\sum_k\alpha^k(\epsilon^4)X^k(\epsilon^4))_{i,m}X_{m}^4(\epsilon^4) $$

for arbitary $t$:

$$ \lambda^t(\epsilon^t)X_{i}^1(\epsilon^t) = A(\epsilon^t,\sum_k\alpha^k(\epsilon^t)X^k(\epsilon^t))_{i,m}X_{m}^t(\epsilon^t) $$
$$ \lambda^t(\epsilon^t)X_{i}^1(\epsilon^t) = A(\epsilon^t,\sum_k\left( \lim_{n\rightarrow\infty} \frac{\lambda^k(\epsilon^k)^n\alpha^k(\epsilon^k)}{\sum_p\lambda^p(\epsilon^p)^n\alpha^p(\epsilon^p)}X^k(\epsilon^t) \right))_{i,m}X_{m}^t(\epsilon^t) $$

making the $\epsilon$ arbitrary, noting that for all other species $\partial A / \partial \epsilon =0$:

$$ \lambda^t(\epsilon)X_{i}^1(\epsilon) = A(\epsilon,\sum_k\left( \lim_{n\rightarrow\infty} \frac{\lambda^k(\epsilon)^n\alpha^k(\epsilon)}{\sum_p\lambda^p(\epsilon)^n\alpha^p(\epsilon)}X^k(\epsilon^t) \right))_{i,m}X_{m}^t(\epsilon) $$

... what about if we view multiple species as being a A00A block matrix, with redundancy and degenerate maximal eigenvalues.



deriving wrt $\epsilon^1$ gives:

$$ \lambda^{1\prime}(\epsilon^1)X_{i}^1(\epsilon^1) + \lambda^1(\epsilon^1)X_{i}^{1\prime}(\epsilon^1) = A(\epsilon^1,\sum_k\alpha^k(\epsilon^1)X^k(\epsilon^1))_{i,m}X_{m}^1(\epsilon^1) $$




deriving wrt $\epsilon$:
$$ \frac{\lambda(\epsilon)}{\partial \epsilon}X_{i}(\epsilon) + \lambda(\epsilon)\frac{X_{i}(\epsilon)}{\partial \epsilon} = $$
$$\frac{A(\epsilon,X(\epsilon))_{i,m}}{\partial \epsilon}X_{m}(\epsilon)
 + \frac{A(\epsilon,X(\epsilon))_{i,m}}{\partial X_{k}}\frac{\partial X_{k}(\epsilon)}{\partial \epsilon}X_{m}(\epsilon)
 + A(\epsilon,X(\epsilon))_{i,m}\frac{X_{m}(\epsilon)}{\partial \epsilon} $$

We consider the equality evaluated at $\epsilon=0$, letting shorthand:
$$X(\epsilon)_{i}=X_i, \frac{X_i(\epsilon)}{\partial \epsilon}=X'_i, \lambda(\epsilon)=\lambda, \frac{\lambda(\epsilon)}{\partial \epsilon}=\lambda'$$
$$A(\epsilon,X(\epsilon))_{i,m} = A_{i,m}$$
$$\frac{A(\epsilon,X(\epsilon))_{i,m}}{\partial X_{k}} = B_{i,m,k}$$
$$\frac{A(\epsilon,X(\epsilon))_{i,m}}{\partial \epsilon} = C_{i,m}$$
 thus:\\
$$\lambda'X_i+\lambda X'_i = C_{i,m}X_m + B_{i,m,k}X'_kX_m +A_{i,m}X'_m $$

$$\lambda'X_i+\lambda I_{i,m}X'_m - B_{i,k,m}X_kX'_m - A_{i,m}X'_m = C_{i,m}X_m $$
where $I$ is identity matrix, thus:
$$\lambda'X_i+\left(\lambda I_{i,m} - B_{i,k,m}X_k - A_{i,m}\right)X'_m = C_{i,m}X_m $$
Letting $\bar{X} = \bigl[ \begin{smallmatrix}X' \\\hline \lambda' \end{smallmatrix}\bigr]$ then
$$\left[\begin{smallmatrix}\lambda I_{i,m} - B_{i,k,m}X_k - A_{i,m} & \rvline & X_i \end{smallmatrix}\right]\bar{X}_m = C_{i,m}X_m $$
Adding a normalising constraint that $\sum_i X'_{i}=0$ gives:
$$\left[\begin{smallmatrix}\lambda I_{i,m} - B_{i,k,m}X_k - A_{i,m} & \rvline & X_i \\\hline 1 & \rvline & 0\end{smallmatrix}\right]\bar{X}_m = \left[\begin{smallmatrix} C_{i,m}X_m \\\hline 0\end{smallmatrix}\right] $$

Which can be inverted (or pseudo-inverted) to find $\bar{X}$ yay
\end{document}
